@article{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
eprint = {1508.06615},
journal = {Aaai},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@article{Koo2010,
abstract = {This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98{\%} of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.},
author = {Koo, Terry and Rush, Alexander M. and Collins, Michael and Jaakkola, Tommi and Sontag, David},
journal = {Emnlp},
keywords = {Dependency Parsing,Dual Decomposition,Non-Projective},
number = {October},
pages = {1288--1298},
title = {{Dual Decomposition for Parsing with Non-Projective Head Automata}},
year = {2010}
}
@article{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
eprint = {1509.00685},
isbn = {9781941643327},
journal = {In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
number = {September},
pages = {379--389},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@article{Rush2011,
abstract = {We describe an exact decoding algorithm for syntax-based statistical translation. The ap- proach uses Lagrangian relaxation to decom- pose the decoding problem into tractable sub- problems, thereby avoiding exhaustive dy- namic programming. Themethod recovers ex- act solutions, with certificates of optimality, on over 97{\%} of test examples; it has compa- rable speed to state-of-the-art decoders.},
author = {Rush, Alexander M and Collins, Michael},
journal = {Computational Linguistics},
pages = {72--82},
title = {{Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation}},
url = {http://www.aclweb.org/anthology/P11-1008},
year = {2011}
}
@article{Rush,
abstract = {Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for de-pendency parsing using linear-time vine prun-ing and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their un-pruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an un-pruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.},
author = {Rush, Alexander M and Petrov, Slav},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rush, Petrov - Unknown - Vine Pruning for Efficient Multi-Pass Dependency Parsing.pdf:pdf},
title = {{Vine Pruning for Efficient Multi-Pass Dependency Parsing}}
}
@article{Rush2012,
abstract = {Dual decomposition, and more generally Lagrangian relaxation, is a classical method for com- binatorial optimization; it has recently been applied to several inference problems in natural lan- guage processing (NLP). This tutorial gives an overview of the technique. We describe example al- gorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of com- binatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.},
author = {Rush, Alexander M. and Collins, Michael},
journal = {Journal of Artificial Intelligence Research},
pages = {305--362},
title = {{A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing}},
volume = {45},
year = {2012}
}
@article{Rush2010,
abstract = {This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.},
author = {Rush, Alexander M. and Sontag, David and Collins, Michael and Jaakkola, Tommi},
journal = {Emnlp},
keywords = {Dual Decomposition,Linear Programming},
number = {October},
pages = {1--11},
title = {{On dual decomposition and linear programming relaxations for natural language processing}},
url = {http://dl.acm.org/citation.cfm?id=1870658.1870659},
year = {2010}
}
@unpublished{Schmaltz2016,
abstract = {We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.},
archivePrefix = {arXiv},
arxivId = {1604.04677},
author = {Schmaltz, Allen and Kim, Yoon and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {arxiv},
eprint = {1604.04677},
title = {{Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction}},
url = {http://arxiv.org/abs/1604.04677},
year = {2016}
}
@article{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M.},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1502.05698},
isbn = {1502.05698},
issn = {1502.05698},
journal = {arXiv preprint},
keywords = {boring formatting information, machine learning, I},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}
@article{Wiseman2015,
abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa- tions, and we report the best overall score on the CoNLL 2012 English test set to date.},
author = {Wiseman, Sam and Rush, Alexander M and Shieber, Stuart M and Weston, Jason},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
pages = {1416--1426},
title = {{Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution}},
year = {2015}
}
@inproceedings{Wiseman2016,
abstract = {There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.},
archivePrefix = {arXiv},
arxivId = {1604.03035},
author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {To appear: NAACL-2016},
eprint = {1604.03035},
title = {{Learning Global Features for Coreference Resolution}},
url = {http://arxiv.org/abs/1604.03035},
year = {2016}
}
@article{Kim2016a,
abstract = {Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.},
archivePrefix = {arXiv},
arxivId = {1606.07947},
author = {Kim, Yoon and Rush, Alexander M.},
eprint = {1606.07947},
file = {::},
month = {jun},
title = {{Sequence-Level Knowledge Distillation}},
url = {http://arxiv.org/abs/1606.07947},
year = {2016}
}
@article{Strobelt2016,
abstract = {Recurrent neural networks, and in particular long short-term memory networks (LSTMs), are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows a user to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with domain specific structural annotations. We further show several use cases of the tool for analyzing specific hidden state properties on datasets containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis.},
archivePrefix = {arXiv},
arxivId = {1606.07461},
author = {Strobelt, Hendrik and Gehrmann, Sebastian and Huber, Bernd and Pfister, Hanspeter and Rush, Alexander M.},
eprint = {1606.07461},
file = {::},
month = {jun},
title = {{Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.07461},
year = {2016}
}
@article{Wiseman2016a,
abstract = {Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1606.02960},
author = {Wiseman, Sam and Rush, Alexander M.},
eprint = {1606.02960},
file = {::},
month = {jun},
title = {{Sequence-to-Sequence Learning as Beam-Search Optimization}},
url = {http://arxiv.org/abs/1606.02960},
year = {2016}
}
